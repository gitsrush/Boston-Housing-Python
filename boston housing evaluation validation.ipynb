{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "065fe094-8a75-480f-b681-a04086750804",
    "_uuid": "59acabfcefd926029c6d5fd53aebe2b5f23968b4"
   },
   "source": [
    "# Predicting Boston Housing Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "786a480c-1a31-459f-81fd-a561b3e4f14e",
    "_uuid": "cfbe27c88e5cd562ac10686d7d9568551b0e0b95",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import scorer\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "data = pd.read_csv('../input/housing.csv')\n",
    "prices = data['MEDV']\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "    \n",
    "# Success\n",
    "print(\"Boston housing dataset has {} data points with {} variables each.\".format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f0b95a7b-5573-43d8-9851-c40581bd6265",
    "_uuid": "ca2b7e8cc394aa47a15f08c317288e67d48fb1e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for data types and Null values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "796d61c8-82fb-4d3b-ac61-46cac73dbc5c",
    "_uuid": "8d5e1d2b19da8a9c0e76ef53cc8114677d4f55bd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa4cb167-0b3c-400c-8b9e-21bd7dc54209",
    "_uuid": "46238b753aaf948b88871c3d1d774a2f0d840f07"
   },
   "source": [
    "## Visualization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5459cac-daeb-48d3-b811-7eaa27142451",
    "_uuid": "6f70d1a27e2b02305ea40df767050c8065c8be88",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Suppress matplotlib user warnings\n",
    "# Necessary for newer version of matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\n",
    "#\n",
    "# Display inline matplotlib plots with IPython\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "###########################################\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "\n",
    "def ModelLearning(X, y):\n",
    "    \"\"\" Calculates the performance of several models with varying sizes of training data.\n",
    "        The learning and testing scores for each model are then plotted. \"\"\"\n",
    "    \n",
    "    # Create 10 cross-validation sets for training and testing\n",
    "    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n",
    "    cv.get_n_splits(X)\n",
    "\n",
    "    # Generate the training set sizes increasing by 50\n",
    "    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n",
    "\n",
    "    # Create the figure window\n",
    "    fig = pl.figure(figsize=(10,7))\n",
    "\n",
    "    # Create three different models based on max_depth\n",
    "    for k, depth in enumerate([1,3,6,10]):\n",
    "        \n",
    "        # Create a Decision tree regressor at max_depth = depth\n",
    "        regressor = DecisionTreeRegressor(max_depth = depth)\n",
    "\n",
    "        # Calculate the training and testing scores\n",
    "        sizes, train_scores, test_scores = learning_curve(regressor, X, y, \\\n",
    "            cv = cv, train_sizes = train_sizes, scoring = 'r2')\n",
    "        \n",
    "        # Find the mean and standard deviation for smoothing\n",
    "        train_std = np.std(train_scores, axis = 1)\n",
    "        train_mean = np.mean(train_scores, axis = 1)\n",
    "        test_std = np.std(test_scores, axis = 1)\n",
    "        test_mean = np.mean(test_scores, axis = 1)\n",
    "\n",
    "        # Subplot the learning curve \n",
    "        ax = fig.add_subplot(2, 2, k+1)\n",
    "        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n",
    "        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n",
    "        ax.fill_between(sizes, train_mean - train_std, \\\n",
    "            train_mean + train_std, alpha = 0.15, color = 'r')\n",
    "        ax.fill_between(sizes, test_mean - test_std, \\\n",
    "            test_mean + test_std, alpha = 0.15, color = 'g')\n",
    "        \n",
    "        # Labels\n",
    "        ax.set_title('max_depth = %s'%(depth))\n",
    "        ax.set_xlabel('Number of Training Points')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlim([0, X.shape[0]*0.8])\n",
    "        ax.set_ylim([-0.05, 1.05])\n",
    "    \n",
    "    # Visual aesthetics\n",
    "    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n",
    "    fig.suptitle('Decision Tree Regressor Learning Performances', fontsize = 16, y = 1.03)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def ModelComplexity(X, y):\n",
    "    \"\"\" Calculates the performance of the model as model complexity increases.\n",
    "        The learning and testing errors rates are then plotted. \"\"\"\n",
    "    \n",
    "    # Create 10 cross-validation sets for training and testing\n",
    "    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n",
    "    cv.get_n_splits(X)\n",
    "\n",
    "    # Vary the max_depth parameter from 1 to 10\n",
    "    max_depth = np.arange(1,11)\n",
    "\n",
    "    # Calculate the training and testing scores\n",
    "    train_scores, test_scores = validation_curve(DecisionTreeRegressor(), X, y, \\\n",
    "        param_name = \"max_depth\", param_range = max_depth, cv = cv, scoring = 'r2')\n",
    "\n",
    "    # Find the mean and standard deviation for smoothing\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot the validation curve\n",
    "    pl.figure(figsize=(7, 5))\n",
    "    pl.title('Decision Tree Regressor Complexity Performance')\n",
    "    pl.plot(max_depth, train_mean, 'o-', color = 'r', label = 'Training Score')\n",
    "    pl.plot(max_depth, test_mean, 'o-', color = 'g', label = 'Validation Score')\n",
    "    pl.fill_between(max_depth, train_mean - train_std, \\\n",
    "        train_mean + train_std, alpha = 0.15, color = 'r')\n",
    "    pl.fill_between(max_depth, test_mean - test_std, \\\n",
    "        test_mean + test_std, alpha = 0.15, color = 'g')\n",
    "    \n",
    "    # Visual aesthetics\n",
    "    pl.legend(loc = 'lower right')\n",
    "    pl.xlabel('Maximum Depth')\n",
    "    pl.ylabel('Score')\n",
    "    pl.ylim([-0.05,1.05])\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "def PredictTrials(X, y, fitter, data):\n",
    "    \"\"\" Performs trials of fitting and predicting data. \"\"\"\n",
    "\n",
    "    # Store the predicted prices\n",
    "    prices = []\n",
    "\n",
    "    for k in range(10):\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "            test_size = 0.2, random_state = k)\n",
    "        \n",
    "        # Fit the data\n",
    "        reg = fitter(X_train, y_train)\n",
    "        \n",
    "        # Make a prediction\n",
    "        pred = reg.predict([data[0]])[0]\n",
    "        prices.append(pred)\n",
    "        \n",
    "        # Result\n",
    "        print(\"Trial {}: ${:,.2f}\".format(k+1, pred))\n",
    "\n",
    "    # Display price range\n",
    "    print(\"\\nRange in prices: ${:,.2f}\".format(max(prices) - min(prices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6abfd31b-69b7-4a8c-bd63-3842636c3208",
    "_uuid": "46dacee7bbb7983aa0c41459ef34487a7f9096c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimum price of the data\n",
    "minimum_price = np.min(prices)\n",
    "\n",
    "# Maximum price of the data\n",
    "maximum_price = np.max(prices)\n",
    "\n",
    "# Mean price of the data\n",
    "mean_price = np.mean(prices)\n",
    "\n",
    "# Median price of the data\n",
    "median_price = np.median(prices)\n",
    "\n",
    "# Standard deviation of prices of the data\n",
    "std_price = np.std(prices)\n",
    "\n",
    "# Show the calculated statistics\n",
    "print(\"Statistics for Boston housing dataset:\\n\")\n",
    "print(\"Minimum price: ${:,.2f}\".format(minimum_price))\n",
    "print(\"Maximum price: ${:,.2f}\".format(maximum_price))\n",
    "print(\"Mean price: ${:,.2f}\".format(mean_price))\n",
    "print(\"Median price ${:,.2f}\".format(median_price))\n",
    "print(\"Standard deviation of prices: ${:,.2f}\".format(std_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79e7f520-8293-4f19-bf5a-ab74ea566310",
    "_uuid": "c1acad2155c40258910cf6c0d0fad121e9136706",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('float_format', '{:.2f}'.format):\n",
    "    print(prices.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f4b6e4eb-32a6-4e02-89aa-808762e9367e",
    "_uuid": "0dfa1a876dea4dff732c6e7f5317477fba7e1935",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(18,4))\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    axs[i].set_ylim([0, 1200000])\n",
    "# RM vs Price\n",
    "_ = sns.regplot(x=features['RM'], y=prices, ax=axs[0])\n",
    "# LSSAT vs Price\n",
    "_ = sns.regplot(x=features['LSTAT'], y=prices, ax=axs[1])\n",
    "# PTRATIO vs Price\n",
    "_ = sns.regplot(x=features['PTRATIO'], y=prices, ax=axs[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "69531a94-3780-4997-bb02-c5e3ba7d9796",
    "_uuid": "945db06320d25b461e10f11cd89d1c7a72489269",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    \n",
    "    # Calculate the performance score between 'y_true' and 'y_predict'\n",
    "    score = r2_score(y_true, y_predict)\n",
    "    \n",
    "    # Return the score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b1be649-173a-4509-886b-5f60cd664689",
    "_uuid": "72a1c382cd7e887c916484e3de303fdb5eb07b8d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the performance of this model\n",
    "score = performance_metric([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\n",
    "print(\"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba1f8289-8157-4278-b7a4-7aede829ddd6",
    "_uuid": "61085a36dd1d0f4db9ee422221d6b3c7bbc343d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, prices, test_size=0.2, random_state=0)\n",
    "\n",
    "# Success\n",
    "print(\"Training and testing split was successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b978865-146b-4d1c-ae74-893cab098e88",
    "_uuid": "b19bba1270e2c9a286c01bf89706b8dcf809a18d",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Produce learning curves for varying training set sizes and maximum depths\n",
    "ModelLearning(features, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49988328-8ee0-4ca9-910b-c00fe6142b6d",
    "_uuid": "02c73ed114da9c42e9da716fe8b91ec04c92d373",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ModelComplexity(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fbf14f95-524b-4a36-ac3f-7ee2bb9caa24",
    "_uuid": "a910011f92f0b72cc7a664eac42af0f7b436b6a1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(X, y):\n",
    "    \"\"\" Performs grid search over the 'max_depth' parameter for a \n",
    "        decision tree regressor trained on the input data [X, y]. \"\"\"\n",
    "    \n",
    "    # Create cross-validation sets from the training data\n",
    "    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n",
    "    cv_sets.get_n_splits(X)\n",
    "\n",
    "    # Create a decision tree regressor object\n",
    "    regressor = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
    "    params = {'max_depth':range(1,11)}\n",
    "\n",
    "    # Transform 'performance_metric' into a scoring function using 'make_scorer' \n",
    "    scoring_fnc = make_scorer(performance_metric)\n",
    "\n",
    "    # Create the grid search cv object --> GridSearchCV()\n",
    "    # Make sure to include the right parameters in the object:\n",
    "    # (estimator, param_grid, scoring, cv) which have values 'regressor', 'params', 'scoring_fnc', and 'cv_sets' respectively.\n",
    "    grid = GridSearchCV(regressor, params, scoring_fnc, cv=cv_sets)\n",
    "\n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "216e7791-de3a-4ded-b341-399a55598a88",
    "_uuid": "a260d23ca1ff9072326f46e3bc7f417d807167ed",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the training data to the model using grid search\n",
    "reg = fit_model(X_train, y_train)\n",
    "\n",
    "# Produce the value for 'max_depth'\n",
    "print(\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.best_estimator_.get_params()['max_depth']))\n",
    "print(\"Best Score is {:.2f}\".format(reg.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7a111b99-1046-4275-b31e-1a12a3a96ae8",
    "_uuid": "f96c281c919a2fb51fabc15afea20d5fe3ce3034",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('float_format', '{:.2f}'.format):\n",
    "    print(prices.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db1527a3-c5ad-4e4d-aaaf-c1d3c8cc9339",
    "_uuid": "7f35439f481ec4cbfa661c533794a594c4770fc3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('float_format', '{:.2f}'.format):\n",
    "    print(features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83d6061a-4581-4b16-b79d-230d1e07eaf2",
    "_uuid": "1887891df10182c1f03d9c03fc4391dd03417b9c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Produce a matrix for client data\n",
    "client_data = [[5, 17, 15], # Client 1\n",
    "               [4, 32, 22], # Client 2\n",
    "               [8, 3, 12]]  # Client 3\n",
    "\n",
    "# Show predictions\n",
    "for i, price in enumerate(reg.predict(client_data)):\n",
    "    print(\"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
